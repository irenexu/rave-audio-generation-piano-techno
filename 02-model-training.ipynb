{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Model Training\n",
    "\n",
    "The model is trained based on the instructions on https://github.com/acids-ircam/RAVE (RAVE: A variational autoencoder for fast and high-quality neural audio synthesis by Antoine Caillon and Philippe Esling).\n",
    "\n",
    "Training a RAVE model usually involves 3 steps: dataset preparation, model training, and model export.\n",
    "\n",
    "It is recommended to train the model through Terminal directly and using TensorBoard to monitor Traning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAVE Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation, using `rave train --helpfull` to check all the parameters\n",
    "\n",
    "Key Parameters: \n",
    "- batch: Batch size (default: '8')\n",
    "- channels: number of audio channels (default: '0')\n",
    "- ckpt: Path to previous checkpoint of the run\n",
    "- config: RAVE configuration to use (default: \"['v2.gin']\")\n",
    "- db_path: Preprocessed dataset path\n",
    "- out_path: Output folder (default: 'runs/')\n",
    "- gpu: GPU to use\n",
    "- max_steps: Maximum number of training steps (default: '6000000')\n",
    "- n_signal: Number of audio samples to use during training (default: '131072')\n",
    "- name: Name of the run\n",
    "- save_every: save every n steps (default: '500000')\n",
    "- val_every: Checkpoint model every n steps (default: '10000')\n",
    "- workers: Number of workers to spawn for dataset loading (default: '8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install torch torchvision torchaudio\n",
    "# pip install wget\n",
    "# pip install acids-rave\n",
    "# conda install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard - Monitor Training\n",
    "\n",
    "Re-visit TensorBoard to access all the metrics is not possible if you do not have all the checkpoints (which have been removed from this repository due to the GitHub file size limitation).\n",
    "\n",
    "A separate local repository has been zipped and uploaded,  which can be used to access all the metrics of the training. Please download using the link provided: \n",
    "- piano model:\n",
    "- techno model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Preparation\n",
    "\n",
    "Prepare the dataset using the following command:\n",
    "\n",
    "`rave preprocess --input_path $dataset --output_path $preprocessed_dataset --channels $channels`\n",
    "\n",
    "- input_path: the location of your preprocessed dataset (e.g., sounds/preprocessed_audio/piano-dataset.mp3)\n",
    "- output_path: the destination path where you want to save the dataset after it has been prepared for training\n",
    "- channels: the number of audio channels in the input files. \n",
    "  - It is recommended to use mono (1 channel) for compatibility, as the current version of RAVE has issues processing multichannel audio and may not be able to export a stereo version successfully.\n",
    "  - Processing multichannel audio can significantly increase training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my example on mac\n",
    "! rave preprocess --input_path '/Users/irenex/Documents/GitHub/ai4media-project/sounds/preprocessed_audio/piano-dataset.mp3' --output_path '/Users/irenex/Documents/GitHub/ai4media-project/sounds/rave_processed_audio/piano' --channels 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "\n",
    "Start training the model using the following command:\n",
    "\n",
    "`rave train --config $config --db_path /dataset/path --out_path /model/out --name $name_of_your_model --channels X`\n",
    "\n",
    "[Key parameters]\n",
    "- config: model architecture selected for training, e.g., v2\n",
    "  - RAVEv2 has many different configurations. The most common used one is v2 at the moment, which is an mproved continuous model (faster, higher quality)\n",
    "- dp_path: the location of rave-processed dataset (should be the same as the output_path in 1. Dataset Prepartion, unless you moved directory \n",
    "- out_path: the destination path where you want to save the model and its checkpoints \n",
    "- name: the name of your model\n",
    "- channels: the number of input audio channels \n",
    "\n",
    "[Optional parameters]\n",
    "- val_every: checkpoint model every n steps (default is 10000)\n",
    "- batch: Batch size (default is 8)\n",
    "  - though it is possible to increase batch size on a powerful GPU, it does not automatically make a better model, the optimal batch size needs be determined with experiments, as different optimal batch size is needed for different traning stage (smaller may be better for the first traning stage, and larger may be  better for the second training state)\n",
    "  -  it is suggested to using the default setting of 8 to ensure a rather good convergence (at least for first time users)\n",
    "  - there are lots of discussions reagarding batch size\n",
    "  - https://github.com/acids-ircam/RAVE/issues/234\n",
    "  - https://github.com/acids-ircam/RAVE/issues/22 \n",
    "  - https://discord.com/channels/987249093124452400/1120704270812053595 \n",
    "- gpu: explicitly set the gpu that will be used for this training (e.g., gpu -1 means all gpu; gpu 0 means using the first gpu)\n",
    "- workers: define the number of workers to spawn for dataset loading (default is 8, can be changed to other number depending on the number of cpus on your machine - doesn't really make a difference in training speed in my case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I used Nvidia GeForce RTX 4080 to train the model, though RAVE is already faster in training compared to other models, it still takes at least 3 days on a GPU to train a complete model (normally a good model needs at least 3 million steps, according to Discord); training on cpu is not recomended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my example on wins\n",
    "! rave train --config v2 --db_path '/Users/irenex/Documents/GitHub/ai4media-project/sounds/rave_processed_audio/piano' --out_path '/Users/irenex/Documents/GitHub/ai4media-project/rave_ckpt/piano' --name rav_piano --channels 1 --workers 16 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Resume Training\n",
    "\n",
    "RAVE automatically save checkpoints for every 10000 steps (default setting), training can be resumed from any selected checkpoints.\n",
    "\n",
    "Resume training the model using the following command:\n",
    "\n",
    "`rave train --config $config --db_path /dataset/path --name $name_of_your_model --ckpt /dataset/ckpt --out_path /model/out --channels X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my example on wins\n",
    "! rave train --config v2 --db_path '/Users/irenex/Documents/GitHub/ai4media-project/sounds/rave_processed_audio/piano' --name rav_piano --ckpt '/Users/irenex/Documents/GitHub/ai4media-project/rave_ckpt/rav_piano_e18d54798e/version_0/checkpoints' --out_path '/Users/irenex/Documents/GitHub/ai4media-project/rave_ckpt/piano' --channels 1 --workers 16 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Export\n",
    "\n",
    "Once trained, the model can be exported to a torchscript (.ts) file using the following command:\n",
    "\n",
    "`rave export --run /path/to/your/run (--streaming)`\n",
    "\n",
    "*Setting the --streaming flag will enable cached convolutions, making the model compatible with realtime processing\n",
    "\n",
    "Different checkpoints can export different models, which can be useful in model comparison (in different steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my example on Mac\n",
    "! rave export --run /Users/irenex/Desktop/ravmodel/rav_techno_e18d54798e/epoch4059/version_2/ --streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
